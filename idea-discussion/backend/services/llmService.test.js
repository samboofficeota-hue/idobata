import OpenAI from "openai";
import { afterEach, beforeEach, describe, expect, it, vi } from "vitest";
import { callLLM, testLLM } from "./llmService"; // Adjust the path if necessary

// Mock the OpenAI client
vi.mock("openai", () => {
  // Mock the default export (the OpenAI class constructor)
  const MockOpenAI = vi.fn();
  // Mock the chat.completions.create method
  MockOpenAI.prototype.chat = {
    completions: {
      create: vi.fn(),
    },
  };
  return {
    default: MockOpenAI, // Ensure this matches how OpenAI is imported in llmService.js
  };
});

// Mock process.env
const originalEnv = { ...process.env };

describe("llmService", () => {
  let mockCreate;

  beforeEach(() => {
    // Reset mocks and environment variables before each test
    vi.resetAllMocks();
    process.env = { ...originalEnv, OPENAI_API_KEY: "test-key" }; // Ensure API key is set for tests

    // Get a reference to the mocked create function for easier use
    // Note: This assumes OpenAI is instantiated only once in llmService.js
    // If it's instantiated multiple times, this approach needs adjustment.
    // Since it's instantiated at the module level, this should be fine.
    const MockOpenAIInstance = new OpenAI(); // Create an instance to access the mocked method
    mockCreate = MockOpenAIInstance.chat.completions.create;
  });

  afterEach(() => {
    // Restore original environment variables
    process.env = originalEnv;
  });

  describe("callLLM", () => {
    const messages = [{ role: "user", content: "Test prompt" }];

    it("should call the OpenAI API with correct parameters for text output", async () => {
      const mockResponse = {
        choices: [{ message: { content: "Test response" } }],
      };
      mockCreate.mockResolvedValue(mockResponse);

      const response = await callLLM(messages);

      expect(mockCreate).toHaveBeenCalledTimes(1);
      expect(mockCreate).toHaveBeenCalledWith({
        model: "google/gemini-2.0-flash-001", // Default model
        messages: messages,
      });
      expect(response).toBe("Test response");
    });

    it("should call the OpenAI API with correct parameters for JSON output", async () => {
      const mockJsonResponse = { test: "data" };
      const mockResponse = {
        choices: [{ message: { content: JSON.stringify(mockJsonResponse) } }],
      };
      mockCreate.mockResolvedValue(mockResponse);

      const response = await callLLM(messages, true);

      expect(mockCreate).toHaveBeenCalledTimes(1);
      expect(mockCreate).toHaveBeenCalledWith({
        model: "google/gemini-2.0-flash-001",
        messages: [
          {
            role: "user",
            content: "Test prompt\n\nPlease respond ONLY in JSON format.", // Check if prompt is modified
          },
        ],
        response_format: { type: "json_object" },
      });
      expect(response).toEqual(mockJsonResponse);
    });

    it("should parse JSON response enclosed in ```json ... ```", async () => {
      const mockJsonResponse = { key: "value" };
      const rawContent = `\`\`\`json\n${JSON.stringify(mockJsonResponse)}\n\`\`\``;
      const mockResponse = {
        choices: [{ message: { content: rawContent } }],
      };
      mockCreate.mockResolvedValue(mockResponse);

      const response = await callLLM(messages, true);
      expect(response).toEqual(mockJsonResponse);
    });

    it("should parse JSON response enclosed in ``` ... ```", async () => {
      const mockJsonResponse = { another: "test" };
      const rawContent = `\`\`\`\n${JSON.stringify(mockJsonResponse)}\n\`\`\``;
      const mockResponse = {
        choices: [{ message: { content: rawContent } }],
      };
      mockCreate.mockResolvedValue(mockResponse);

      const response = await callLLM(messages, true);
      expect(response).toEqual(mockJsonResponse);
    });

    it("should throw an error if JSON parsing fails when jsonOutput is true", async () => {
      const invalidJsonContent = "This is not JSON.";
      const mockResponse = {
        choices: [{ message: { content: invalidJsonContent } }],
      };
      mockCreate.mockResolvedValue(mockResponse);

      await expect(callLLM(messages, true)).rejects.toThrow(
        `LLM did not return valid JSON. Raw response: ${invalidJsonContent}`
      );
    });

    it("should throw an error if LLM returns empty content", async () => {
      const mockResponse = {
        choices: [{ message: { content: null } }], // Simulate empty content
      };
      mockCreate.mockResolvedValue(mockResponse);

      await expect(callLLM(messages)).rejects.toThrow(
        "LLM returned empty content."
      );
    });

    it("should use the specified model", async () => {
      const specificModel = "openai/gpt-4";
      const mockResponse = {
        choices: [{ message: { content: "Response from GPT-4" } }],
      };
      mockCreate.mockResolvedValue(mockResponse);

      await callLLM(messages, false, specificModel);

      expect(mockCreate).toHaveBeenCalledWith(
        expect.objectContaining({
          model: specificModel,
        })
      );
    });

    it("should handle errors from the OpenAI API call", async () => {
      const apiError = new Error("API Error");
      mockCreate.mockRejectedValue(apiError);

      await expect(callLLM(messages)).rejects.toThrow("API Error");
    });
  });

  describe("testLLM", () => {
    it("should call callLLM and log the response", async () => {
      const mockResponseContent = "Hello from LLM!";
      const mockResponse = {
        choices: [{ message: { content: mockResponseContent } }],
      };
      mockCreate.mockResolvedValue(mockResponse);

      // Spy on console.log
      const consoleSpy = vi.spyOn(console, "log");

      const result = await testLLM();

      expect(mockCreate).toHaveBeenCalledTimes(1);
      expect(mockCreate).toHaveBeenCalledWith({
        model: "google/gemini-2.0-flash-001", // Default model for testLLM
        messages: [{ role: "user", content: "Hello!" }],
      });
      expect(result).toBe(mockResponseContent);
      expect(consoleSpy).toHaveBeenCalledWith("Testing LLM connection...");
      expect(consoleSpy).toHaveBeenCalledWith(
        "LLM Test Response (default model):"
      );
      expect(consoleSpy).toHaveBeenCalledWith(mockResponseContent);

      // Clean up spy
      consoleSpy.mockRestore();
    });

    it("should log an error and return if OPENAI_API_KEY is missing", async () => {
      process.env.OPENAI_API_KEY = undefined; // Remove API key for this test

      // Spy on console.error
      const consoleErrorSpy = vi.spyOn(console, "error");

      const result = await testLLM();

      expect(result).toBeUndefined(); // Function should return early
      expect(mockCreate).not.toHaveBeenCalled(); // API should not be called
      expect(consoleErrorSpy).toHaveBeenCalledWith(
        "OPENAI_API_KEY not found in environment variables. Make sure .env is loaded correctly from the project root."
      );

      // Clean up spy
      consoleErrorSpy.mockRestore();
    });

    it("should handle errors during the testLLM call", async () => {
      const testError = new Error("Test call failed");
      mockCreate.mockRejectedValue(testError);

      // Spy on console.error
      const consoleErrorSpy = vi.spyOn(console, "error");

      await expect(testLLM()).rejects.toThrow("Test call failed");
      expect(consoleErrorSpy).toHaveBeenCalledWith(
        "LLM Test Failed:",
        testError
      );

      // Clean up spy
      consoleErrorSpy.mockRestore();
    });

    it("should allow specifying a model for testLLM", async () => {
      const specificModel = "openai/gpt-3.5-turbo";
      const mockResponseContent = "Hello from GPT-3.5!";
      const mockResponse = {
        choices: [{ message: { content: mockResponseContent } }],
      };
      mockCreate.mockResolvedValue(mockResponse);
      const consoleSpy = vi.spyOn(console, "log");

      const result = await testLLM(specificModel);

      expect(mockCreate).toHaveBeenCalledTimes(1);
      expect(mockCreate).toHaveBeenCalledWith(
        expect.objectContaining({ model: specificModel })
      );
      expect(result).toBe(mockResponseContent);
      expect(consoleSpy).toHaveBeenCalledWith(
        `LLM Test Response (${specificModel}):`
      );

      consoleSpy.mockRestore();
    });
  });
});
